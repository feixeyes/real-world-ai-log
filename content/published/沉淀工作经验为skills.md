# 我是如何把工作经验，沉淀成 Codex CLI Skills 的

## 一开始的体验：不够"惊艳"

最近一段时间，我在实际工作中持续使用 Codex CLI 写代码。

一开始的体验并不算"惊艳"。不是说它不好用，而是没有那种"哇，这也太智能了"的感觉。很多时候，我还是要反复解释需求、纠正方向、提醒它注意细节。

直到我意识到一个问题：

真正影响效果的，不是模型本身，而是我和它的协作方式。

## 转折点：问题不在模型，而在协作方式

在多次使用后，我逐渐形成了一套相对稳定的工作流程：

1. 先让 Codex 充分理解当前项目
2. 再分析新功能或 issue 的影响范围
3. 制定整体计划
4. 我确认后，再拆解成小 task 执行
5. 每个 task 完成后，进行充分检查和测试

这套流程跑通之后，效果明显好了很多。

但问题是——我几乎每次都要把这些要求重新输入一遍。

既低效，也没法持续优化。每次都是临时补充、临时调整，缺少稳定的基准。

## Before：重复输入的，其实不是 Prompt，而是"方法"

回过头看，我发现自己反复输入的，并不是某几句固定的 Prompt。

而是一整套隐含的工作方法：

- 不要一上来就写代码
- 先理解项目上下文
- 明确改动影响范围
- 有计划、有节奏地推进
- 每一步都可检查、可回滚

这些东西，对人来说是"常识"，但对 Codex 来说，如果不明确说出来，它不会默认遵守。

于是我开始思考：

**能不能把这些要求，变成 Codex 的"默认行为"？**

## After：把工作流程，沉淀成 Skills

后来我给 Codex CLI 增加了两个自定义 skills：`add-feature` 和 `fix-issue`。

它们并不复杂，但各自都内置了一套明确的协作流程。

以 `add-feature` 为例，大致包含几个阶段：

**阶段 1：理解项目** — 先深入理解当前项目结构和已有逻辑，不急着写代码

**阶段 2：分析影响范围** — 分析新增功能可能涉及的模块、识别潜在风险和依赖

**阶段 3：制定计划** — 给出完整实现计划，列出需要修改的文件和步骤

**阶段 4：等待确认** — 等我确认后，再把计划拆成小 task，不会直接动手

**阶段 5：执行与检查** — 每个 task 完成后检查和测试，发现问题立即调整

这样一来，我不再需要反复解释"我希望你怎么做"，只需要告诉 Codex：**现在是 add-feature 场景**。

它就知道该按什么节奏配合我。

## 真正的变化：从"用 AI"到"训练协作方式"

一开始我以为，Skills 的最大价值是省时间。后来发现，更重要的是另一点：**它们可以被持续优化**。

比如：

- 某次发现拆 task 拆得不够细，导致一个 task 里改了太多文件
- 或者测试阶段考虑不充分，漏掉了边界情况
- 或者某类项目需要额外的检查步骤，比如类型检查或 lint

我不会在 Prompt 里临时补一句，而是直接去调整 Skill 本身。

久而久之，这些 Skills 变成了**我个人工作习惯的外化版本**。

Codex 也越来越像一个熟悉我做事方式的搭档——不是在"帮我写代码"，而是在"按我习惯的方式协作"。

## 持续演进与写在最后

回头看这个过程，我最大的感受是：

使用 AI 的关键，不是一次性写出"完美 Prompt"，而是不断把隐性的判断和流程**显性化、结构化**。

Skills 的意义，也不只是 Codex CLI 的一个功能，而是一种思路：

- 把重复决策变成默认行为
- 把个人经验沉淀为系统能力
- 把临时对话，变成可复用协作模式

这件事一旦开始做，就会形成正反馈。

对我来说，**AI 实践记录**的价值，就在于这些细节。

不是模型有多强，而是我如何一步步，把 AI 真正嵌进自己的工作方式里。

接下来，我会持续记录这些 Skills 的演进过程：哪些设计是多余的，哪些约束是必须的，以及它们在真实项目中的效果变化。

如果你也在使用 Codex 或其他 Coding Agent，下次用 AI 的时候，不妨留意一下：

**你有没有在重复说同样的话？**
